{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('pushkin.csv')[:1000]\n",
    "data2 = pd.read_csv('dostoewsky.csv')[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def superrost(words):\n",
    "    return ''.join(filter(lambda x: ord(x) in range(ord('а'), ord('я')+1) or \n",
    "                           ord(x) in range(ord('А'), ord('Я')+1) or \n",
    "                           x == ' ', list(words.replace('\\n', ' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print('1')\n",
    "data.text = data.text.apply(superrost)\n",
    "print('2')\n",
    "data2.text = data2.text.apply(superrost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "analyzer = MorphAnalyzer()\n",
    "\n",
    "def preprocess(text):\n",
    "    w = text.lower().split()\n",
    "\n",
    "\n",
    "    filtered_words = [word for word in w if word not in stopwords.words('russian')]\n",
    "\n",
    "    words = tokenizer.tokenize(' '.join(filtered_words))\n",
    "    \n",
    "\n",
    "    for i in range(len(words)):\n",
    "        words[i] = analyzer.parse(words[i])[0].normal_form\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print('1')\n",
    "data['text'] = data.text.apply(preprocess)\n",
    "print('2')\n",
    "data2['text'] = data2.text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "print('concat')\n",
    "fitted = pd.concat([data.text,data2.text], axis =0)\n",
    "print('fitting')\n",
    "vectorizer.fit(fitted)\n",
    "print('1')\n",
    "vectorized = vectorizer.transform(data.text) \n",
    "print('2')\n",
    "vectorized2 = vectorizer.transform(data2.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus=(vectorized)-(vectorized2)\n",
    "minus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sums = minus.toarray().sum(axis=0)\n",
    "sums.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def beword(word):\n",
    "    key = 'dict.1.1.20191123T212452Z.411f68444aa87cf0.0625a3c576f183071005d9e3474f48970456140d'\n",
    "    url = 'https://dictionary.yandex.net/api/v1/dicservice.json/lookup?key={}&lang=ru-ru&text={}'.format(key, word)\n",
    "    answer = requests.get(url)\n",
    "    if answer.status_code != 200:\n",
    "        raise Exception('Server err: {}'.format(answer.text))\n",
    "    if json.loads(answer.text)['def'] == []:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def get_a_word_translation(key: str) -> str:\n",
    "    URL_AUTH = 'https://developers.lingvolive.com/api/v1.1/authenticate'\n",
    "    URL_TRANSLATE = 'https://developers.lingvolive.com/api/v1/Minicard'\n",
    "    KEY = 'OTlkY2Q4MDItZWJlNi00ZWI0LTkyYWYtMThmMTFiNGM2N2FiOjY4MDhkNjVmYTA3MzQ2ZGRiMmQzNDc3YzUyNTA0NTNi'\n",
    "    \n",
    "    headers_auth = {'Authorization': 'Basic ' + KEY}\n",
    "    auth = requests.post(URL_AUTH, headers=headers_auth)\n",
    "    if auth.status_code == 200:\n",
    "        token = auth.text\n",
    "        headers_translate = {\n",
    "            'Authorization': 'Bearer ' + token\n",
    "        }\n",
    "        params = {\n",
    "            'text': key,\n",
    "            'srcLang': 1049,\n",
    "            'dstLang': 1049\n",
    "        }\n",
    "        req = requests.get(\n",
    "            URL_TRANSLATE, headers=headers_translate, params=params)\n",
    "        res = req.json()\n",
    "        try:\n",
    "            value = res['Translation']['Translation']\n",
    "            return value\n",
    "        except TypeError:\n",
    "            if res == 'Incoming request rate exceeded for 50000 chars per day pricing tier':\n",
    "                raise Exception(res)\n",
    "            else:\n",
    "                return None\n",
    "    else:\n",
    "        raise Exception('Status code not 200: {}'.format(auth.status_code))\n",
    "\n",
    "\n",
    "def superbeword(word):\n",
    "    if get_a_word_translation(word)==None:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "li = []\n",
    "for i in range(len(vectorizer.get_feature_names())):\n",
    "    word = vectorizer.get_feature_names()[i]\n",
    "    if (sums[i] > (sums.mean() - 100)) and (len(word)<=25):\n",
    "        #if (beword(word) or superbeword(word)):\n",
    "        li.append(word)\n",
    "li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
